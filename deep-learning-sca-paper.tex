%% bare_jrnl_seminar.tex
%%
%% This is a slightly modified version of the original IEEEtran example bare_jrnl.tex 
%% to meet the needs for the "advanced seminar for security in information technology"
%% at the institute for security in information technology, TUM. 
%% This template is also applicable for writing German texts.
%% 
%% April 2011, Hermann Seuschek
%%

\documentclass[journal]{IEEEtran}
%\documentclass[10pt,        % Don't change the font size!
%               a4paper,     % Don't change the paper size!
%               journal,     % Journal paper format
%               draft       % Enable this parameter to get a draft version.
%               ]{IEEEtran}
\makeatletter


\def\markboth#1#2{\def\leftmark{\@IEEEcompsoconly{\sffamily}\MakeUppercase{\protect#1}}%
\def\rightmark{\@IEEEcompsoconly{\sffamily}\MakeUppercase{\protect#2}}}
\makeatother

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  \usepackage{tikz}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi

% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
\interdisplaylinepenalty=2500

% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/

% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/

%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/

%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/

% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.

%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/

% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/

%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.

%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley and Jeff Goldberg.
% This package may be useful when used in conjunction with IEEEtran.cls'
% captionsoff option. Some IEEE journals/societies require that submissions
% have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.3.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% For subfigure.sty:
% \let\MYorigsubfigure\subfigure
% \renewcommand{\subfigure}[2][\relax]{\MYorigsubfigure[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat/subfig command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.

% *** PDF, URL AND HYPERLINK PACKAGES ***
\usepackage{url}
% \url{my_url_here}.

% Referencing paragraphs/pictures using \autoref after assigning \label
\usepackage{hyperref}
\hypersetup{colorlinks=false}

\usepackage{import}
\graphicspath{images/}
\usepackage[section]{placeins}


% properly print units, enable compact product between units
\usepackage[inter-unit-product =\cdot]{siunitx}
% load units \bit, \byte usw
\sisetup{detect-weight=true, binary-units=true}

%\usepackage[backend=biber,sortlocale = auto, isbn = false, doi=false, citestyle = numeric-comp,firstinits=true]{biblatex}
%\addbibresource{literature.bib}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Using Deep Learning Techniques to augment Side Channel Analysis}
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Nimar~Blume}% <-this % stops a space
%\thanks{Fabrizio Desantis, Chair of Security in Information Technology of the Technical University of Munich}% <-this % stops a space

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.

% The paper headers
%\markboth{Hauptseminar Sicherheit in der Informationstechnik, Sommersemester 2011}%
\markboth{Advanced Seminar for Security in Information Technology, Summer Term 2017}%
{Nimar Blume: Using Deep Learning Techniques to augment Side Channel Analysis}

%\markboth{Journal of \LaTeX\ Class Files,~Vol.~6, No.~1, January~2007}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.

% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2007 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}

% make the title area
\maketitle

\begin{abstract}
%\boldmath
At the moment, cryptography is everywhere, allowing endpoints to transmit data securely over an insecure network. A multitude of encryption algorithms are available but even those which have no proven weakness theoretically can be attacked by attacking the algorithm's implementation. Therefore, leaking side channel data such as a chip's power consumption can be used to infer parts of or the entire secret key. \\
Currently, the Template Attack (TA) is the most powerful side channel attack (SCA), requiring the least amount of side channel data in the attack phase. However, recently Machine Learning (ML) based TAs were shown to supersede traditional TAs in efficiency. Since Deep Learning (DL) techniques have beaten ML techniques in for example object recognition in images, this paper examines viability and performance of DL techniques in SCAs. In the end, especially Convolutional Neural Networks have proven to be more effective than traditional SCA techniques such as TAs.
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the journal you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals frown on math
% in the abstract anyway.

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
deep learning, neural network, side channel analysis, side channel attack, encryption, AES
\end{IEEEkeywords}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
% \IEEEpeerreviewmaketitle

\section{Introduction}
Data encryption is commonly employed to restrict data access to selected people or endpoints. Proper encryption provides privacy, integrity and authenticity of data exchanged between two or more parties over an insecure connection. An encryption key or a pair of keys is generated with which the chosen data is encrypted, so that the data can only be read or modified by an endpoint who possesses the appropriate secret key. For symmetric encryption, where the same secret key is used to encrypt and decrypt data, the key has to be shared over a secure connection beforehand, whereas asymmetric encryption uses key pairs, one of which is used to encrypt and one is used to decrypt data. This paper will only consider attacking symmetric encryption, although techniques discussed could also partly be applied to asymmetric encryption algorithms. \\Encryption algorithms have been attacked in multiple ways, and attacking the algorithms' implementations has proven to be effective against cryptographically sound algorithms. In the past, machine learning (ML) has been used to attack implementations as part of the profiling stage in Template attacks (\autoref{subsec:template}), where Support Vector Machines (SVMs) have been trained to rank key probabilities. While deep learning (DL) techniques have been initially developed in the 1970s, they only recently made a comeback due to the availability of GPUs which enable training of large, many layered Neural Networks in reasonable time and because less computationally intensive activation functions have been found. As a matter of fact, DL techniques have developed to consistently outperform ML techniques in areas such as image recognition (\cite{cnn-beats-svm:alex}), therefore applying DL techniques to break cryptographic algorithms is attractive to investigate. In the end, consistent with the improvement seen in image recognition, DL techniques improve ML based attacks and require as low as half the traces needed for ML based attack to select the correct secret key in a side channel attack on AES (\autopageref{sc:aes}).  

\subsection{The Advanced Encryption Standard}
\label{sc:aes}
While there are many encryption algorithms available, this paper focuses on the most popular symmetric block cipher: the Advanced Encryption Standard (henceforth AES) with a \SI{128}{\bit} key. The AES algorithm encrypts data in blocks of \SI{128}{\bit} or \SI{16}{\byte}. Should a block be smaller than \SI{16}{\byte}, padding will be appended until the block size reaches \SI{16}{\byte}. Furthermore, AES expands a single \SI{128}{\bit} key to 11 round keys which are then used in the subsequent 11 rounds to encrypt the given data. It is important to note, that a compromise of any of the 11 round keys enables an attacker to reconstruct the initial \SI{128}{\bit} key and thus decrypt the whole encrypted data set. Further details can be found in the AES proposal paper: \cite{aes:Rijndael_1999}.

\subsection{Side Channel Analysis}
Side Channel Analysis (henceforth SCA) refers to a technique used to break encryption schemes by using data indirectly generated by the implementation of the cryptographic algorithm, instead of attacking the algorithm itself. Even a theoretically perfectly safe cryptographic algorithm can be subject to SCA and be broken by it. SCA uses side channels such as the power consumption of a microprocessor, electro magnetic emissions or even emitted sound to reconstruct the entire or parts of the secret key used to encrypt the data. It can be possible to infer the secret key form side channel data due to data dependant program flows. Specifically, the code contains conditional statements acting on the secret key data. For example a certain loop will only execute if the currently processed key bit is zero, therefore the power consumption of the microprocessor will measurably increase. Furthermore, power modelling is used to predict a microprocessor's power consumption based on the secret key data. Power models such as Hamming Weight (a "1" consumes more energy to be processed than a "0") or Hamming Distance (bitwise comparison resulting in the number of different bits) are popular choices. Therefore, to protect against SCA it is vital to pay attention not only to the theoretical safety of an encryption algorithm but also to its implementation.

\section{The state of Side Channel Analysis}
Side channel attacks (SCAs) were first developed in 1996 by Paul Kocher \cite{first-sca:kocher} in the form of timing attacks. Timing attacks can exploit an implementation's execution flow dependence on secret key data. Later, SCAs have been extended to infer secret key data by analysing a processor's power consumption and its dependence on secret key data. 

\subsection{Simple Power Analysis}
Simple Power Analysis (henceforth SPA) is a SCA based on the power consumption of the target device. The target device (e.g. a smart card) performs multiple encryption or decryption operations during which the attacker measures the device's power consumption. The attacker does not know the plaintext, which makes it a ciphertext only attack. Then, the attacker calculates a hypothetical power consumption for each possible key combination using for example the Hamming Weight model. Finally, he correlates the hypothetical power consumption to the measured power traces and then ranks the hypothetical keys accordingly. The correct key should now be among the top ranked hypothetical keys. However, SPA is very reliant on clean power traces. Modern computers perform many operations in parallel and thus generate a lot of noise, which is why SPA is best used on simple devices like smart cards. 

\subsection{Differential Power Analysis}
Differential Power Analysis (henceforth DPA) is similar to SPA because it also tries to extract the secret key from a device by using power traces taken during a operation. However, in contrast to the SPA, multiple power traces are taken during different device states. Power traces of the target device are also taken while the device is idle to determine the background noise. The noise level is then subtracted from the power traces taken during the cryptographic operation, resulting in a more distinct signal. Still, concurrent operations running parallel to the cryptographic operations worsen the quality of the power traces, but that can be compensated by taking a large number of traces and averaging over them. Thus, DPA can also be used on more complex devices than smart cards. Further, High-order DPA can also be used to attack even masked implementations by combining traces measured at different points in a device, but will not be discussed further in this paper.

\subsection{Template Attacks}
\label{subsec:template}
A Template Attack is a very powerful, targeted attack on an implementation of a cryptographic algorithm. The premise is, that the attacker has restricted access to the target machine, but ubiquitous access to similar machines containing a varying secret key. The attacker can now execute many encryption operations while varying the secret key and recording power traces. The goal is to acquire a large labelled data set, which maps power consumption to key bits on the specific type of machine. Afterwards, uses the Gaussian assumption probability density function (pdf) of $ f_z(L|Z=z) $ (see below) to map the sensitive values $ z $ to power traces $ L $ taken while the sensitive value was processed. Finally, the attacker obtains a small number of power traces from the target machine containing the secret key during an encryption or decryption operation and feeds the power trace or relevant points of interest extracted from it ($ l $) along with the key hypotheses $ k^{*} $ into the pdf for analysis, where the possibility $ p_k^{*},l=f_z(a_j) $ denotes the chance that based on $ l $, $ k $ constitutes the correct key. 

$$ f_z(L|Z=z) \simeq \dfrac{1}{(2\pi)^{d}det(\Sigma_z)}exp(-\frac{1}{2}(L-\mu_z)^{T}\Sigma_z(L-\mu_z)) $$

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

\section{Theory on Deep Learning techniques for SCA}
\subsection{Using Deep Learning over Machine Learning}
DL techniques differentiate themselves from ML techniques in that ML relies on features selected by the attacker manually, for example power traces are correlated with hypothetical key values and subsequently points of interest (POIs) chosen based on the correlation. Using DL techniques, the input consists of raw data and the NN chooses the POIs itself. In fact, applying a principal component analysis before feeding the data into the NN for dimensionality reduction reduces the NN's effectiveness as shown in \cite[p.~15]{breaking-crypto-dl:prouff} due to possibly removing vital information to achieve lower dimensional data. \\
This paper will first present a set of different DL techniques which have been proven to be effective in various applications such as computer vision and natural language processing and subsequently discuss their performance in \autoref{sec:exp-res}.

\subsection{Multi layer perceptron}
\label{sc:mlp}
A Multilayer perceptron (MLP) is a Neural Network (NN) built out of perceptrons, which are single neuron representations often used in computer science. MLPs are also often called Fully Connected Layers (FCLs), and are often used in NNs as an output layer due to their gloobal mixing property as each perceptron is connected to every other perceptron. This is especially interesting for CNNs (\autoref{ssec:cnn}), as convolutions only mix local areas, with the reach depending on the kernel size used in the filter. A perceptron consists of one or multiple inputs which are multiplied by respective weights and then fed into an activation function. The activation function then determines the output of a single perceptron. A popular choice for an activation function is the rectified linear unit (ReLU): $ f(x)=max(0, x) $, because the output is not limited to a value between "1" and "0" only, and thus can represent a real value opposed to expressing a probability only. A MLP consists of a number of single perceptrons ordered into layers, of which each perceptron is connected to every perceptron in the following layer as seen in \autoref{fig:mlp}. When training MLPs, the predicted output of a perceptron in the last layer $ \hat{Y}_i $ is compared to the desired output $ Y_i $ and the error is calculated: $ E_i = (\hat{Y}_i - Y_i)^{2} $. The gradient descend algorithm is then used to adjust the weights step by step to minimise the error and thus train the NN.

\begin{figure}[ht]
	\centering
	\def\svgwidth{\columnwidth}
	\import{images/}{mlp.pdf_tex}
	\caption[A MLP built out of multiple single perceptrons]{A MLP constructed out of multiple single perceptrons}
	\label{fig:mlp}
\end{figure}

\subsection{Convolutional Neural Networks}
\label{ssec:cnn}
Convolutional Neural Networks (henceforth CNN) are Neural Networks which have recently risen in popularity due to their strength in classifying images (\cite{cnn-beats-svm:alex}). They are good at extracting features from input data represented in a 2nd order tensor invariant from the feature's scale, rotation or vicinity. Feature maps are created in the profiling phase from the input data through convolutional layers until a reasonably small and thus dense feature map is created. The extracted feature maps do not resemble features which commonly are extracted manually. \autoref{fig:cnn} shows the basic structure of a CNN, where the filters of the convolution operation are trained using the back propagation algorithm. Furthermore, a Normalisation layer can be inserted before the FCL to reduce the impact of the FCL's weights on the output due to high input values into the FCL.\\
% TODO: CHECK THIS PARAGRAPH FOR TRUTH
During the attack phase, power traces corresponding to one secret key are then fed into the CNN and the feature maps will consequently output the key deemed most likely to be correct among the possible hypothetical keys. 

\begin{figure}[ht]
	\centering
	\def\svgwidth{\columnwidth}
	\import{images/}{cnn.pdf_tex}
	\caption[Short description of a CNN]{Layer structure of a CNN}
	\label{fig:cnn}
\end{figure}

\subsubsection{Convolutional layer}
The Convolutional layer of a CNN is responsible for extracting features from the inputs. A feature map is extracted from the input data by convoluting a filter tensor with the input tensor. The filter is a 2nd order tensor of a smaller size than the input and usually initialised with small random variables which are adjusted in the training phase through the gradient descend algorithm. After one convolutional layer the size of the output decreases but effectively still retains most of the information in the feature map.  

\subsubsection{Fully connected layer}
\label{sssec:fcl}
A fully connected layer (FCL) is a layer of a Multilayer perceptron (\autoref{sc:mlp}). FCL are commonly used as output layer in a CNN because it is very easy to tune it to the desired number of outputs per calculation cycle. Furthermore, it mixes the outputs across the whole NN because each perceptron is connected to one another. 

\subsubsection{Pooling layer}
Pooling layers further reduce the size of the previous stacked layers' outputs by dividing the feature maps into larger regions and concatenating the regions' maximum values to produce a more dense feature map as seen in \autoref{fig:cnn}.
 
\subsubsection{Normalisation layer}
Normalisation layers, also sometimes called Softmax layers can be used to convert the previous layers' outputs values into a probability distribution relative to each other. 

\subsection{Stacked Auto-Encoders}
Stacked Auto-Encoders (AE) are another type of Neural Network which consist of an encoder layer followed by multiple FCL (\autoref{sssec:fcl}) and at the end feature a decoder layer. During training the goal of an AE is to reproduce the given input at the output. Its weights are then trained by gradually adding noise to the input data while the network should still output the original, denoised input. The FCLs are generally sized smaller than the number of inputs as to extract a more compact feature set from which the input data can then be recreated. Finally, after training multiple encoding layers, the decoding layer at the end is removed and the trained encoding layers are stacked onto one another to produce a NN containing the previously trained feature descriptors. Auto encoders can thus also be used for feature extraction but in this case were tested with respect to finding patterns in the cryptographic operations to rank possible keys in the most likely order. 

\subsection{Recurrent Neural Networks}
Recurrent Neural Networks (RNNs) and its varieties such as Long and Short Term Memory Units (LSTM) are NNs build similarly to MLPs with the most important difference being that RNNs have a memory unit at each perceptron. Thus it is able to store a state and refer to it in a later point in time. That property is very good for data which is connected across many inputs, for example natural languages where a word in the beginning of a sentence can influence the grammatical correctness of a word placed at the sentence's end. However, SCAs usually do not deal with that dimension of data as for example the power consumption during processing a key byte should not leak data about the subsequent byte. Thus, the expected performance for RNNs in SCA is not very high, as the processed data is usually rather independent.

\subsection{Using Deep Learning to improve Side Channel Analysis}
Deep Learning assists Side Channel Analysis in that it provides a new method of profiling, and subsequently a new engine for attacking as well. DL techniques can be used in Template Attacks (\autoref{subsec:template}) in the profiling phase instead of the Gaussian assumption of the leakage. A NN is then trained instead using the available labelled data set of power traces with corresponding key data \cite{nn-aes:gilmore}. First, power traces are recorded during cryptographic operations which use the key data on the profiling device. In this case, 1000 traces per key byte were recorded. Afterwards, the Side Channel Attack itself is performed in a similar manner to the Template Attack \autoref{subsec:template}. 

\section{Experimental results}
\label{sec:exp-res}
H. Maghrebi et. compared the performance of Template Attacks against Deep Learning based attacks in \cite{breaking-crypto-dl:prouff}. In those experiments, the first SBox of AES was targeted. The AES implementation was not masked and data made available by the DPA Contest V2 \cite{dpacv2:web} was used. In the profiling phase, 1000 power traces per sensitive key value were utilised and the attack phase consisted of the average of 10 attacks using 2000 power traces each. All attacks were performed on the same FPGA board. \\
SCA techniques based on DL using Auto Encoders, MLPs with and without previous feature extraction through Principal Content Analysis (PCA), CNNs and Long Short Term Memory Units were compared against statistical Template Attacks. \\ 
\autoref{fig:dlpaper-results} shows the evolution of the rank of the correct key, where rank 1 denotes the most likely correct key among all key hypotheses over the number of traces used during the attack phase for comparison (\cite{sca-framework:standaert}). Thus, the attack performs better if fewer power traces are required to get the correct key ranked 1st. \\
It is noticeable that DL based techniques fair well, CNN, AE and MLP based attacks outperform the Template Attack with the CNN based attack ranking the correct key 1st with only half of the traces required by the classical Template Attack. Furthermore, the MLP based attack performs better without pre-selecting features through PCA which suggests that the NNs feature selection is superior for this use case and the NN uses the slight variations in input data which are discarded by the PCA to reduce the data's dimensionality. It is also interesting to note, that DL techniques are good at ranking the correct key highly even without using many traces during the attack phase, therefore DL techniques might allow powerful one shot attacks. 

\begin{figure}[ht]
	\centering
	\def\svgwidth{\columnwidth}
	\import{images/}{dlpaper-results-png.pdf_tex}
	\caption[Performance comparison of SCA techniques used on the AES]{Performance comparison of SCA techniques used on AES, source: \cite[p.~16]{breaking-crypto-dl:prouff}}
	\label{fig:dlpaper-results}
\end{figure}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.

% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}

% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.

\section{Conclusion}
In the end, Deep Learning techniques can improve SCA significantly if employed correctly as indicated in \autoref{fig:dlpaper-results} by requiring up to only half of the attack traces needed for a successful TA. By focussing on and specifically tuning the CNN parameters (number of layers and entities per layer) it might be possible to gain an even bigger advantage on Template Attacks. The experimental data also shows that it is essential to pick the appropriate DL technique for the working data set to get the best or even a better result than without using DL techniques, that means that the input data has to be well understood in order to be able to chose a CNN instead of e.g. a LSTM. Understanding the data set is even more important to make the decision if pre-processing like feature extraction on the data is advisable before using the data to train a NN. Moreover, DL techniques also improve the rank of the correct key among the hypotheses when only few power traces are available during the attack phase, whereas the traditional TA performs significantly worse with very few traces available. \\
Furthermore, NNs such as Recurrent Neural Networks and its varieties have not been discussed in this paper in depth but might be interesting in applications where the obtained power traces are not as clean and well defined and thus could contain dependencies across traces. 

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.

% use section* for acknowledgement
%\section*{Acknowledgment}

%The authors would like to thank...

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,literature}

\end{document}
